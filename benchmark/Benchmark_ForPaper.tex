\documentclass{article}
\usepackage[cm]{fullpage}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{amsmath,amssymb}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\Diag}{Diag}
\DeclareMathOperator*{\TPR}{TPR}
\DeclareMathOperator*{\FPR}{FPR}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\RR}{\mathbb R}

\begin{document}

\title{Benchmarking fpop}
\author{RM, TDH, PF, GR}
\maketitle

\tableofcontents

\section{Empirical evaluation of fpop}

As explained in section \ref{sec:simil-diff-betw} functional pruning
leads to a better pruning in the following sense: any point pruned by
inequality-based pruning will also be pruned by functional pruning.
However, functionnal pruning is computationnaly more demanding than
inequality based pruning.  We thus decided to empirically compared the
performance of FPOP to PELT, pDPA and binary segmentation (binseg).

To do so, we implemented FPOP for the quadratic loss in C++.
More precisely, we considered the cost $C(y_{\tau_j+1:\tau_{j+1}} = \sum_{t=\tau_j+1}^{\tau_{j+1}} (y_t - \bar{y}_{\tau_j+1:\tau_{j+1}})^2,$ where
$ \bar{y}_{\tau_j+1:\tau_{j+1}} = \frac{1}{\tau_{j+1} - \tau_j} \sum_{t=\tau_j+1}^{\tau_{j+1}} y_t$.
We assessed the runtimes of FPOP on both real microarray data as well as synthetic data.

There are several differences between FPOP and the other
algorithms, as shown in the table below.
\begin{center}
  \begin{tabular}{ccccc}
    algorithm & models computed & optimal? & implementation & runtime as a function of segments $K$\\
    \hline
    FPOP & 1 & yes & C++  &  ?                  \\
    PELT & 1 & yes & C++  &  decrease \\
    pDPA & $K_{max}$ & yes & C++ & increase \\
    Binseg & $K_{max}$ & no & C++ & increase
  \end{tabular}
\end{center}

Note that the Binary segmentation heuristic is computationally
extremely fast. That is because its complexity is on average in
$\mathcal{O}(n \log K_{\text{max}})$. Furthermore, it relies on a few
fairly simple operations and hence the constant in front of this big
$\mathcal O$ notation is expected to be quite low.  For this reason we
believe that binary segmentation is good reference point in terms of
speed and we do not think it is possible to be much faster.


\subsection{Speed benchmark: 4467 chromosomes from 
  tumor microarrays}

\citet{HOCKING-SegAnnDB} proposed to benchmark the speed of 
segmentation algorithms on a database of 4467 problems of size varying
from 25 to 153662 data points. These data come from different
microarrays data sets (Affymetrix, Nimblegen, BAC/PAC) and different
tumor types (leukemia, lymphoma, neuroblastoma, medulloblastoma).

We compared fpop to several other segmentation algorithms: pDPA
\citep{pruned-dp}, pelt \citep{pelt}, and binary segmentation
(binseg).  For such data, we expect a small number of changes and for
all profiles we ran pDPA and binseg with a maximum number of changes
$K_{\text{max}}=52$.  In this scenario we expect binary segmentation
to be particularly efficient as its complexity is on average in
$\mathcal{O}(n\log K_{max})$.

We used the R \verb|system.time| function to measure
the execution time of all 4 algorithms on all 4467 segmentation
problems. The R source code for these timings is in
\verb|benchmark/systemtime.arrays.R| in the opfp project repository on
R-Forge:
\url{https://r-forge.r-project.org/projects/opfp/}


It can be seen in Figure~\ref{fig:sys_runtimes_microarray} that in
general fpop is faster than pelt and pDPA, but about two times slower
than binseg.  Note that \verb|system.time| is inaccurate for small
times, as pointed out in Figure~\ref{fig:sys_runtimes_microarray}
middle and right.  For these small profiles we also assessed the
performances of fpop, pelt and binseg using the \verb|microbenchmark|
package and confirmed that fpop is faster than pelt. For these small
problems we were surprised to observe that fpop exhibited about the
same speed as binseg (microbenchmark results figure not shown).
 
\begin{figure}\label{fig:sys_runtimes_microarray}
 \parbox{7.5cm}{ 
   \includegraphics[width=\linewidth]{figure-systemtime-arrays-small}
  }
  \parbox{11.5cm}{  
    \includegraphics[width=\linewidth]{figure-systemtime-arrays-fpop-pelt-small}
  }
  %\parbox{6cm}{ \includegraphics[height=5cm]{figure-systemtime-arrays-fpop-multiBinSeg}}
  \caption{Left) Runtimes of fpop, pelt, pDPA and Binseg as a function
    of the length of the profile using on the neuroblastoma tumor micro array
    benchmark.  Middle) Runtimes of pelt and fpop for the same
    profiles.  Right) Runtimes of Binseg and fpop for the same
    profiles.  }
\end{figure}





\subsection{Speed benchmark: simulated data with different 
  number of changes}

The speed of Pelt, Binseg and pDPA depends on the underlying number of changes.
For pDPA and Binseg the relationship is clear as to cope with a larger number of changes
one need to increase the maximum number of changes to look for $K_{max}$. 
For a fixed size signal the runtime depency is expected to be in $\log(K)$ for Binseg and
in $K$ for pDPA.

For PELT the relationnship is less clear, however we expect pruning to be more efficient
if there is a large number of change-points. Hence for a fixed size signal we expect 
the runtime of Pelt to improve with the underlying number of change.

Based on section \ref{sec:simil-diff-betw} we expect fpop to be more efficient than pelt and pDPA.
Thus it seems reasonable to expect fpop to be efficient for the whole range of $K$. 
This is what we empirically checked in this section.

To do that we simulated Gaussian signal with 200000 data points and varied the number of changes
and timed the algorithms for each of these signals. We then repeat the same experience for signals with $10^7$ and timed fpop and Binseg only.
The R source code
for these timings is in \verb|benchmark/systemtime.simulation.R| in
the opfp project repository on R-Forge: \url{https://r-forge.r-project.org/projects/opfp/}.

It can be seen on figure \ref{fig:simu_numberK} that fpop is always faster than pDPA and pelt. 
Interestingly for both $n=2\times 10^5$ and $10^7$ it is faster than binseg for a true number of change-point larger than 500.


\begin{figure}\label{fig:simu_numberK}
  \begin{center}
    \includegraphics[width=8cm]{figure-systemtime-simulation-small}
    \includegraphics[width=8cm]{figure-systemtime-simulationLarge-small}
  \end{center}
\caption{Runtimes as a function of the true number of change-points.
Left) For fpop, pelt,
pDPA and Binseg and $n=2. 10^5$
Right) For Binseg and fpop and $n=10^7$
}
\end{figure}



\subsection{Accuracy benchmark: the neuroblastoma data set}

\begin{table}
  \centering
  \parbox{3.5in}{
    \includegraphics[width=\linewidth]{figure-neuroblastoma-roc}
  }
  \parbox{3in}{
    \input{figure-neuroblastoma-test-error}
  }
  \caption{Accuracy results on the neuroblastoma data set.
    \textbf{Left:} 
    Train error ROC curves and counts of incorrect labels
    (FP: false positive, FN: false negative).
    \textbf{Right:} 
    test error (mean and standard deviation over 6 test folds). 
    The ``default'' algorithms ignore the labels in the train set; 
    other algorithms pick the parameter with minimum error with respect
    to the labels in the train set.}
  \label{tab:neuroblastoma-test-error}
\end{table}

\citet{HOCKING-breakpoints} proposed to benchmark the breakpoint
detection accuracy of segmentation models by using annotated regions
defined by experts when they visually inspected scatterplots of the
data. The neuroblastoma data are a set of 575 copy number microarrays
of neuroblastoma tumors, and each chromosome is a separate
segmentation problem. The benchmark is to use a training set of $n$
annotated chromosomes to learn a segmentation model, and then quantify
the annotation error on a test set. Let $d_1, \dots, d_n$ be the
number of data points to segment on each chromosome in the training
data set, and let $\mathbf y_1\in\RR^{d_1}, \dots, \mathbf
y_n\in\RR^{d_n}$ be the vectors of noisy data for each chromosome in
the training set.

Both pelt and pDPA have been applied to this benchmark by first
defining $\beta = \lambda d_i$ for all $i\in\{1, \dots, n\}$, and then
choosing the constant $\lambda$ that maximizes agreement with the
annotated regions in the training set. Since fpop computes the same
segmentation as pelt, we expected the same error rate for
fpop on the neuroblastoma benchmark.

As shown on
\url{http://cbio.ensmp.fr/~thocking/neuroblastoma/accuracy.html}, fpop
(fpop) achieves 2.2\% test error, the same as pDPA (cghseg.k) and pelt
(pelt.n).

In conclusion, the proposed fpop algorithm recovers the same
segmentation as pelt, so has the same breakpoint detection
accuracy.



\bibliographystyle{abbrvnat}
\bibliography{refs}

\end{document}
